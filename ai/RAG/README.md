# LLM 是什么
本质上就是一个函数, 输入是自然语言, 推理, 输出也是自然语言

 - 部署
 - 微调
 - 蒸馏
 - function call
 - mcp

# RAG (Retrieval-Augmented Generation) 生成式检索增强
我们想让大模型知道最近发生的事情, 回答最新的东西

 - 使用搜索引擎
 根据用户输入的问题, 使用搜索引擎搜索相关的内容, 将答案通过提示词讲给大模型作为参考, 大模型根据搜索的内容进行回答。

 比如: 字节有自己的私域搜索引擎, 字节员工为豆包, "抖音部门今年的离职率是多少?", 字节官方构建了自己的向量数据库, 豆包就可以在向量数据库中做数据检索


 - 超长上下文大模型全文嵌入
 比如: moonshot 128k 上下文长度

 - 对结构化、语义化的文件系统结合目录搜索与文件读取
 逐步搜索, AIcoding 工具常用的方式

# 以上三种方式的局限性
 - 搜索引擎搜索的内容是公开的, 并且搭建搜索引擎的成本较高
 - 能接受超长上下文嵌入的大模型的运行成本较高, 其次, 如果需要多轮对话, 每一轮都要带上之前的对话历史, 会增加成本。所以这种手段只支持单轮问答
 - 对结构化搜索, 往往需要多轮执行, 也会消耗大量 token。


# RAG 技术 (推荐使用)
经典的 RAG 技术, 结合了信息检索和语音生成的人工智能架构, 通过将知识内容转换程文本向量, 存放在专门的向量数据库中, 在回答问题时, 让 AI 先查一下向量知识库, 找出相似度高的文本内容, 把它作为参考资料进行回答


# 向量数据库和向量检索
embedding 模型, 将数据压缩成语义向量, 通过向量的相似度(余弦距离, 欧氏距离, 曼哈顿距离, 点积距离) 来进行匹配, 从而真正实现语义化搜索

所以向量搜索才能做到真正意义上的语义匹配。

# 如何构建向量检索系统
1. 选择合适的 embedding 模型 (构建向量的模型: nomic-embed-text) ollama pull nomic-embed-text
2. 准备向量数据库 (vectra: 一个基于 nodeJS 的微型向量数据库) 企业一般使用 node-faiss 来构建向量数据库, faiss 是facebook 开源的一个向量数据库, 支持 CPU 和 GPU 加速
3. pnpm i ollama  (安装 ollama SDK)
4. pnpm i vectra (安装 vectra 向量数据库)
5. 做 RAG 检索将 高相关的内容 作为参考资料, 放入大模型的提示词中, 大模型根据提示词进行回答